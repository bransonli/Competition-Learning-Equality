{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "hCAi_WdoEhPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mRMjgXeOT4js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb14fdf-6166-40d9-99ea-60a891733d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Kgx5292N0MHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('/content/drive/MyDrive/Kaggle/final_keywords.txt', 'r')\n",
        "Lines = file1.readlines()\n",
        "Data = []\n",
        "\n",
        "for line in Lines:\n",
        "    line = line.split()\n",
        "    Data.append(line)"
      ],
      "metadata": {
        "id": "oiam1Dnk_bHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data[0]"
      ],
      "metadata": {
        "id": "clzUf6CqEmIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6d2905-4498-4774-8f59-66c4413f4061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unit', 'enlargement', 'similarity', 'junior', 'high', 'level']"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "encki7kJ3TJd"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=Data, size=20, window=5, min_count=1, workers=4, iter=7)\n",
        "model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "h3E7M6p1S7aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "from nltk import download\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luGncbz8WOKX",
        "outputId": "4c93b2e8-046e-4b2e-c3e4-d9ce75cf5a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"triangles\"))\n",
        "print(lemmatizer.lemmatize(\"circumscribed\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXSaok1WLbp",
        "outputId": "cf53f0f6-5b39-4e5c-de0c-73dac6ef78f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "triangle\n",
            "circumscribed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "# Checking similarity of two words\n",
        "w1 = model.wv.similarity('special', 'relativity')\n",
        "w2 = model.wv.similarity('fraction', 'landmark')\n",
        "w3 = model.wv.similarity('number', 'number')\n",
        "w4 = model.wv.similarity('addition', 'subtraction')\n",
        "w5 = model.wv.similarity('trigonometric', 'algebra')\n",
        "w6 = model.wv.similarity('trigonometry', 'algebra')\n",
        "# Checking similarity of two sentences\n",
        "s1 = model.wv.n_similarity(['triangle', 'three', 'angle'], ['square', 'four', 'side'])\n",
        "s2 = model.wv.n_similarity(['incline', 'plane', 'momentum'], ['listen', 'speak', 'english'])\n",
        "s3 = model.wv.n_similarity(['trigonometric', 'algebra', 'number'], ['cell', 'biology', 'read'])\n",
        "s4 = model.wv.n_similarity(['trigonometry', 'algebra', 'number'], ['cell', 'biology', 'math'])\n",
        "s5 = model.wv.n_similarity(['number', 'algebra', 'addition'], ['positive', 'negative', 'fraction'])\n",
        "\n",
        "print(w1, w2, w3, w4, w5, w6)\n",
        "print(s1, s2, s3, s4, s5)"
      ],
      "metadata": {
        "id": "UKmHh-FCTJBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0d0b03-236b-4739-ca59-5cf92fc258f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6252267 0.01563302 1.0 0.9690129 0.7018216 0.7256734\n",
            "0.78438985 -0.16456425 0.1708943 0.41433692 0.75276726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = ['square', 'triangle', 'trigonometry']\n",
        "sims = model.wv.most_similar(word, topn=10)\n",
        "sims"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4hVjo6jEzkF",
        "outputId": "8595c230-ecc4-4933-8e85-c518401b9c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('polygon', 0.9078943729400635),\n",
              " ('simlar', 0.8990073800086975),\n",
              " ('quadrilateral', 0.8203004002571106),\n",
              " ('pascal', 0.7919774055480957),\n",
              " ('similarity', 0.7888847589492798),\n",
              " ('circle', 0.7697314023971558),\n",
              " ('proof', 0.7499272227287292),\n",
              " ('circumscribed', 0.7498305439949036),\n",
              " ('pythagorean', 0.74430251121521),\n",
              " ('theorem', 0.7425457239151001)]"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model pipeline"
      ],
      "metadata": {
        "id": "V7NsUc9i0GBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "download('stopwords')\n",
        "download('punkt')\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    word_tokens = tokenizer.tokenize(\" \".join(word_tokenize(text)))\n",
        "\n",
        "    filtered_sentence = [lemmatizer.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
        "    return filtered_sentence"
      ],
      "metadata": {
        "id": "tBM7wv5yTKx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given topic give content\n",
        "topics = [] # type text\n",
        "contents =  [] # type text\n",
        "similarities = []\n",
        "pairs = []\n",
        "\n",
        "for topic in topics:\n",
        "    highest_indexes = [0, 0, 0, 0, 0]\n",
        "    topic  = clean(topic) # nltk cleaned\n",
        "    for content in contents:\n",
        "        content = clean(content)\n",
        "        similarity = model.wv.n_similarity(topic, content)\n",
        "        similarities.append(similarity)\n",
        "        for index in highest_indexes:\n",
        "            if similarity > similarities[index]:\n",
        "                index = contents.index(content)\n",
        "                break\n",
        "\n",
        "    temp = ''\n",
        "\n",
        "    for index in highest_indexes:\n",
        "        temp += str(contents[index])\n",
        "\n",
        "\n",
        "    pairs.append([topic, temp])"
      ],
      "metadata": {
        "id": "GzGB1hr4H9OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = np.array([[\"sample1\", \"content1 content2 \"], [\"sample2\", \"content3 content4\"], [\"sample3\", \"content5 content6\"]])\n",
        "\n",
        "topics = pairs[:, 0]\n",
        "contents = pairs[:, 1]\n",
        "\n",
        "submission = pd.DataFrame.from_dict({\n",
        "    'topic_id': topics,\n",
        "    'content_ids': contents,\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv')"
      ],
      "metadata": {
        "id": "CyCPCs7Q7P-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7c5e51-84f8-4761-e22c-134d18f4154d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['content1 \\n content2 \\n' 'content3 \\n content4 \\n'\n",
            " 'content5 \\n content6 \\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy testing (unfinished)"
      ],
      "metadata": {
        "id": "HlCrYxblH-mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate pairs but in id form\n",
        "id_pairs = []\n",
        "topics_id = [] # type id. with similar order as \n",
        "contents_id = [] # type id with similar order as\n",
        "\n",
        "for pair in pairs:\n",
        "    topic_id = topics_id[topics.index(pair[0])]\n",
        "    content_id = contents_id[contents.index(pair[1])]\n",
        "    id_pairs.append([topic_id, content_id])"
      ],
      "metadata": {
        "id": "gfFfG0TDOarU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Converting correlations into array\n",
        "correlations = []\n",
        "corr = pd.read_csv('/content/drive/MyDrive/Kaggle/correlations.csv')\n",
        "topic_ids = corr.topic_id.values.tolist()\n",
        "content_ids = corr.content_ids.values.tolist()\n",
        "\n",
        "\n",
        "for i in range(len(topic_ids)):\n",
        "    correlations.append([topic_ids[i], content_ids[i]])\n",
        "\n",
        "print(correlations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-umt1ByBUwe1",
        "outputId": "20e1d25f-764c-4308-d5b1-2b3257fa0d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c_76231f9d0b5e', 'c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c_ebb7fdf10a7e', 'c_11a1dc0bfb99', 'c_0c6473c3480d c_1c57a1316568 c_5e375cf14c47 c_b972646631cb c_d7a0d7eaf799', 'c_34e1424229b4 c_7d1a964d66d5 c_aab93ee667f4', 'c_7ff92a954a3d c_8790b074383e', 'c_07f1d0eec4b2 c_15a6fb858696 c_175e9db3fc44 c_1c2e804fa58a c_1ec97b588bce c_247c609418f3 c_3a2bf4a358da c_451671e513d2 c_4698dc0a94dc c_4aea397ae55e c_5299d6442ed8 c_6e5ae1f2bb90 c_76a17cfb9d87 c_7868f655c31e c_7cc189e7acb0 c_94e49adc276f c_a035176449cd c_acaa5b1ab542 c_b8d730238789 c_ba964dd99e10 c_bc34d05f7f0a c_c1db2bde6960 c_d26e23e98356 c_d9087b07e2fc c_eef121f3ef45 c_feda60d7d9ee', 'c_2bbc650030f4 c_304ee4f59410', 'c_005e793bd0c5 c_066737fa5146 c_08b2070f92e0 c_0a0f0cfcd01a c_0e3353b058f0 c_0eda77684335 c_0f16d374f415 c_10ca1869c758 c_10dbed87d839 c_11181d0b95aa c_115c5b863574 c_1cd9f807f8e5 c_1f20565b1381 c_222e318d6d8a c_29633da013a9 c_2cd238e941f1 c_30b3480d92f8 c_34bc70d326d5 c_3a12b23519f3 c_3a19935d0e70 c_3bc84cfa9818 c_3cf6ef1121e4 c_470dab588a4d c_48562a9fd7a5 c_4a017a7b88b1 c_5471bafde27d c_5838ba054863 c_5a3a979a3cd9 c_5bac5e027198 c_5f25b381c46a c_5f4b5200a725 c_6062d2e46506 c_60ec15e6b535 c_65e1a1125573 c_683bd6b717fa c_6a1b4e7d6de9 c_6a8228c8357e c_6aef7c2f724c c_6b9542168065 c_6f356bf86b85 c_6fee5dd33c0d c_7265b169bc76 c_738debbd624a c_7971ba5f03c4 c_7d1a59c7044d c_7e2f3739ad0a c_821fc6753fa7 c_8e29ba77df5b c_8ee98ffbb4ec c_91d5815c5c05 c_9520d0abdfe4 c_964f18665f78 c_97f7c60035ea c_a0133f95e907 c_a5545a2ff5ca c_a99a9622f854 c_b44c206ad756 c_b578b555904e c_b6819b9c64df c_b92a55cf1879 c_bebd175342f5 c_bef2c010d6f9 c_bef55777e57b c_c69fc19ab8f2 c_cef9b08c48c4 c_d2cf189bb4ac c_d32102fb335f c_d4f5b6340aa5 c_d5606463f08a c_d937c1b35bc7 c_dcf907ded334 c_def28d2e5eb0 c_df2bcd9b6fcc c_e0b33b9e6118 c_e4262edc8daf c_e7daeadb242c c_e91ac93a2ea4 c_ebde478f5297 c_ecbdf2bb815b c_ee6367296721 c_f16047c6f924 c_f32779fd55bb c_f42f83d45a20 c_f567bb93578f c_f56f6d8aef05 c_f9091a48e912 c_fbd300d2cc72', 'c_0baf72ed7e1e c_5eca28e2cdb4 c_6a5472fb1483 c_ccd6b2522613 c_d4837d0540d9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "wrong = 0\n",
        "\n",
        "for pair in pairs:\n",
        "    if pair[0] in correlations: # problem how to make it so that we only check answer\n",
        "        if pair[1] in correlations[correlations.index(pair[0])][1]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            wrong += 1\n",
        "        \n",
        "\n",
        "print(\"accuracy is\" + str( (correct)/(correct+wrong) ) + \"%\")"
      ],
      "metadata": {
        "id": "x9tqLFL5MXZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"kesehotum\" in \"aaron is a namaste hi kesehotum\""
      ],
      "metadata": {
        "id": "rvTROmpSFlPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69182754-d102-4db7-81ac-25559345c857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}